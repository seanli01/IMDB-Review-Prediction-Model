# IMDB-Review-Prediction-Model

Authors: Brandon Park, Sean Li, Yanis Mouazer

**Abstract**

This project explores the implementation and comparison of Naive Bayes and BERT models in classifying sentiments of IMDB movie reviews. We investigate the trade-offs between traditional machine learning and deep learning methods in terms of training speed, accuracy, and computational resources.

**Introduction** <br>

The project aims to evaluate the performance of Naive Bayes and BERT models in sentiment analysis. While Naive Bayes offers simplicity and faster training times, BERT's deep learning approach provides a more nuanced understanding of language, albeit with higher computational demands.

**Dataset**

The study utilizes a dataset of 50,000 IMDB movie reviews, split evenly for training and testing purposes. The reviews are processed and vectorized for model input, with sentiments labeled as positive or negative.

**Results**
- BERT outperforms Naive Bayes in accuracy but requires more computational resources and time for training.
- Exploration of hyperparameters such as batch size and learning rate in fine-tuning BERT.
- Naive Bayes, despite its simplicity, shows commendable performance in sentiment classification.


**Methodology**
- Implementation of Naive Bayes from scratch and utilization of pre-trained BERT.
- Analysis of models' ability to process and classify text data.
- Fine-tuning of BERT to enhance performance while considering computational constraints.


**Conclusion**
<br>
The study concludes that BERT provides superior accuracy in sentiment analysis but at a higher computational cost compared to Naive Bayes. Future work may explore optimizing these models for different types of textual data and further refine their efficiency.



**References**
For detailed methodologies, results, and further discussions, please refer to the sources cited in the project


